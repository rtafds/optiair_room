{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## leinforcement pyform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.spaces.box import Box\n",
    "\n",
    "import os\n",
    "import PyFoam\n",
    "import PyFoam.FoamInformation\n",
    "from PyFoam.RunDictionary.SolutionDirectory import SolutionDirectory\n",
    "from PyFoam.RunDictionary.ParsedParameterFile import ParsedParameterFile\n",
    "from PyFoam.Basics.DataStructures import Vector\n",
    "from PyFoam.Execution.BasicRunner import BasicRunner\n",
    "from PyFoam.Basics.TemplateFile import TemplateFile\n",
    "import shlex,sys,json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PyFOAM基本コマンド "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"./Allclean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('./Makemesh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method SolutionDirectory.initialDir of <PyFoam.RunDictionary.SolutionDirectory.SolutionDirectory object at 0x7fdbb0d517b8>>\n",
      "uniform (0.5 0 0)\n"
     ]
    }
   ],
   "source": [
    "CASE = SolutionDirectory(\"../aircond2\")\n",
    "print(CASE.initialDir)\n",
    "\n",
    "U = ParsedParameterFile(CASE.initialDir() + \"/U\")\n",
    "print(U['boundaryField']['inlet']['value'])\n",
    "U['boundaryField']['inlet']['value'].setUniform(Vector(0.2,0,0))\n",
    "U.writeFile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/mmt3264/OpenFOAM/mmt3264-6/run/aircond2/0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CASE.initialDir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "COM='''\n",
    "U_400 = ParsedParameterFile(aircondCase.name + \"/400/U\")\n",
    "U_400['boundaryField']['inlet']['value'].setUniform(\n",
    "Vector(0.2,0,0))\n",
    "U_400.writeFile()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OpenFOAMのコマンドを実行\n",
    "args=shlex.split(\"buoyantBoussinesqPimpleFoam -case \" + CASE.name)\n",
    "buoyant=BasicRunner(args,silent=True)\n",
    "summary=buoyant.start()\n",
    "buoyant.runOK()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_latest = ParsedParameterFile(CASE.latestDir() + '/U')\n",
    "U_latest['boundaryField']['inlet']['value'].setUniform(Vector(0.2,0,0))\n",
    "U_latest.writeFile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なコマンド\n",
    "# 最後のケースから再開するコマンド初めからより途中から初めれるといい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "clDict = ParsedParameterFile(CASE.controlDict())\n",
    "clDict['startTime'] = 0\n",
    "clDict.writeFile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 以下、棒倒しのPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 動画の描画関数の宣言\n",
    "# 参考URL http://nbviewer.jupyter.org/github/patrickmineault\n",
    "# /xcorr-notebooks/blob/master/Render%20OpenAI%20gym%20as%20GIF.ipynb\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "from matplotlib import animation\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def display_frames_as_gif(frames):\n",
    "    \"\"\"\n",
    "    Displays a list of frames as a gif, with controls\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0),\n",
    "               dpi=72)\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames),\n",
    "                                   interval=50)\n",
    "\n",
    "    anim.save('movie_cartpole_prioritized_experience_replay.mp4')  # 動画のファイル名と保存です\n",
    "    display(display_animation(anim, default_mode='loop'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# namedtupleを生成\n",
    "from collections import namedtuple\n",
    "\n",
    "Transition = namedtuple(\n",
    "    'Transition', ('state', 'action', 'next_state', 'reward'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定数の設定\n",
    "ENV = 'CartPole-v0'  # 使用する課題名\n",
    "GAMMA = 0.99  # 時間割引率\n",
    "MAX_STEPS = 200  # 1試行のstep数\n",
    "NUM_EPISODES = 500  # 最大試行回数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 経験を保存するメモリクラスを定義します\n",
    "\n",
    "\n",
    "class ReplayMemory:\n",
    "\n",
    "    def __init__(self, CAPACITY):\n",
    "        self.capacity = CAPACITY  # メモリの最大長さ\n",
    "        self.memory = []  # 経験を保存する変数\n",
    "        self.index = 0  # 保存するindexを示す変数\n",
    "\n",
    "    def push(self, state, action, state_next, reward):\n",
    "        '''transition = (state, action, state_next, reward)をメモリに保存する'''\n",
    "\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)  # メモリが満タンでないときは足す\n",
    "\n",
    "        # namedtupleのTransitionを使用し、値とフィールド名をペアにして保存します\n",
    "        self.memory[self.index] = Transition(state, action, state_next, reward)\n",
    "\n",
    "        self.index = (self.index + 1) % self.capacity  # 保存するindexを1つずらす\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        '''batch_size分だけ、ランダムに保存内容を取り出す'''\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        '''関数lenに対して、現在の変数memoryの長さを返す'''\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TD誤差を格納するメモリクラスを定義します\n",
    "\n",
    "TD_ERROR_EPSILON = 0.0001  # 誤差に加えるバイアス\n",
    "\n",
    "\n",
    "class TDerrorMemory:\n",
    "\n",
    "    def __init__(self, CAPACITY):\n",
    "        self.capacity = CAPACITY  # メモリの最大長さ\n",
    "        self.memory = []  # 経験を保存する変数\n",
    "        self.index = 0  # 保存するindexを示す変数\n",
    "\n",
    "    def push(self, td_error):\n",
    "        '''TD誤差をメモリに保存します'''\n",
    "\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)  # メモリが満タンでないときは足す\n",
    "\n",
    "        self.memory[self.index] = td_error\n",
    "        self.index = (self.index + 1) % self.capacity  # 保存するindexを1つずらす\n",
    "\n",
    "    def __len__(self):\n",
    "        '''関数lenに対して、現在の変数memoryの長さを返す'''\n",
    "        return len(self.memory)\n",
    "\n",
    "    def get_prioritized_indexes(self, batch_size):\n",
    "        '''TD誤差に応じた確率でindexを取得'''\n",
    "\n",
    "        # TD誤差の和を計算\n",
    "        sum_absolute_td_error = np.sum(np.absolute(self.memory))\n",
    "        sum_absolute_td_error += TD_ERROR_EPSILON * len(self.memory)  # 微小値を足す\n",
    "\n",
    "        # batch_size分の乱数を生成して、昇順に並べる\n",
    "        rand_list = np.random.uniform(0, sum_absolute_td_error, batch_size)\n",
    "        rand_list = np.sort(rand_list)\n",
    "\n",
    "        # 作成した乱数で串刺しにして、インデックスを求める\n",
    "        indexes = []\n",
    "        idx = 0\n",
    "        tmp_sum_absolute_td_error = 0\n",
    "        for rand_num in rand_list:\n",
    "            while tmp_sum_absolute_td_error < rand_num:\n",
    "                tmp_sum_absolute_td_error += (\n",
    "                    abs(self.memory[idx]) + TD_ERROR_EPSILON)\n",
    "                idx += 1\n",
    "\n",
    "            # 微小値を計算に使用した関係でindexがメモリの長さを超えた場合の補正\n",
    "            if idx >= len(self.memory):\n",
    "                idx = len(self.memory) - 1\n",
    "            indexes.append(idx)\n",
    "\n",
    "        return indexes\n",
    "\n",
    "    def update_td_error(self, updated_td_errors):\n",
    "        '''TD誤差の更新'''\n",
    "        self.memory = updated_td_errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ディープ・ニューラルネットワークの構築\n",
    "# ニューラルネットワークの設定（Chainer風の書き方）\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, n_in, n_mid, n_out):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_in, n_mid)\n",
    "        self.fc2 = nn.Linear(n_mid, n_mid)\n",
    "        self.fc3 = nn.Linear(n_mid, n_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        h2 = F.relu(self.fc2(h1))\n",
    "        output = self.fc3(h2)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# エージェントが持つ脳となるクラスです、PrioritizedExperienceReplayを実行します\n",
    "\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "CAPACITY = 10000\n",
    "\n",
    "\n",
    "class Brain:\n",
    "    def __init__(self, num_states, num_actions):\n",
    "        self.num_actions = num_actions  # CartPoleの行動（右に左に押す）の2を取得\n",
    "\n",
    "        # 経験を記憶するメモリオブジェクトを生成\n",
    "        self.memory = ReplayMemory(CAPACITY)\n",
    "\n",
    "        # ニューラルネットワークを構築\n",
    "        n_in, n_mid, n_out = num_states, 32, num_actions\n",
    "        self.main_q_network = Net(n_in, n_mid, n_out)  # Netクラスを使用\n",
    "        self.target_q_network = Net(n_in, n_mid, n_out)  # Netクラスを使用\n",
    "        print(self.main_q_network)  # ネットワークの形を出力\n",
    "\n",
    "        # 最適化手法の設定\n",
    "        self.optimizer = optim.Adam(\n",
    "            self.main_q_network.parameters(), lr=0.0001)\n",
    "\n",
    "        # TD誤差のメモリオブジェクトを生成\n",
    "        self.td_error_memory = TDerrorMemory(CAPACITY)\n",
    "\n",
    "    def replay(self, episode):\n",
    "        '''Experience Replayでネットワークの結合パラメータを学習'''\n",
    "\n",
    "        # 1. メモリサイズの確認\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        # 2. ミニバッチの作成\n",
    "        self.batch, self.state_batch, self.action_batch, self.reward_batch, self.non_final_next_states = self.make_minibatch(\n",
    "            episode)\n",
    "\n",
    "        # 3. 教師信号となるQ(s_t, a_t)値を求める\n",
    "        self.expected_state_action_values = self.get_expected_state_action_values()\n",
    "\n",
    "        # 4. 結合パラメータの更新\n",
    "        self.update_main_q_network()\n",
    "\n",
    "    def decide_action(self, state, episode):\n",
    "        '''現在の状態に応じて、行動を決定する'''\n",
    "        # ε-greedy法で徐々に最適行動のみを採用する\n",
    "        epsilon = 0.5 * (1 / (episode + 1))\n",
    "\n",
    "        if epsilon <= np.random.uniform(0, 1):\n",
    "            self.main_q_network.eval()  # ネットワークを推論モードに切り替える\n",
    "            with torch.no_grad():\n",
    "                action = self.main_q_network(state).max(1)[1].view(1, 1)\n",
    "            # ネットワークの出力の最大値のindexを取り出します = max(1)[1]\n",
    "            # .view(1,1)は[torch.LongTensor of size 1]　を size 1x1 に変換します\n",
    "\n",
    "        else:\n",
    "            # 0,1の行動をランダムに返す\n",
    "            action = torch.LongTensor(\n",
    "                [[random.randrange(self.num_actions)]])  # 0,1の行動をランダムに返す\n",
    "            # actionは[torch.LongTensor of size 1x1]の形になります\n",
    "\n",
    "        return action\n",
    "\n",
    "    def make_minibatch(self, episode):\n",
    "        '''2. ミニバッチの作成'''\n",
    "\n",
    "        # 2.1 メモリからミニバッチ分のデータを取り出す\n",
    "        if episode < 30:\n",
    "            transitions = self.memory.sample(BATCH_SIZE)\n",
    "        else:\n",
    "            # TD誤差に応じてミニバッチを取り出すに変更\n",
    "            indexes = self.td_error_memory.get_prioritized_indexes(BATCH_SIZE)\n",
    "            transitions = [self.memory.memory[n] for n in indexes]\n",
    "\n",
    "        # 2.2 各変数をミニバッチに対応する形に変形\n",
    "        # transitionsは1stepごとの(state, action, state_next, reward)が、BATCH_SIZE分格納されている\n",
    "        # つまり、(state, action, state_next, reward)×BATCH_SIZE\n",
    "        # これをミニバッチにしたい。つまり\n",
    "        # (state×BATCH_SIZE, action×BATCH_SIZE, state_next×BATCH_SIZE, reward×BATCH_SIZE)にする\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        # 2.3 各変数の要素をミニバッチに対応する形に変形し、ネットワークで扱えるようVariableにする\n",
    "        # 例えばstateの場合、[torch.FloatTensor of size 1x4]がBATCH_SIZE分並んでいるのですが、\n",
    "        # それを torch.FloatTensor of size BATCH_SIZEx4 に変換します\n",
    "        # 状態、行動、報酬、non_finalの状態のミニバッチのVariableを作成\n",
    "        # catはConcatenates（結合）のことです。\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                           if s is not None])\n",
    "\n",
    "        return batch, state_batch, action_batch, reward_batch, non_final_next_states\n",
    "\n",
    "    def get_expected_state_action_values(self):\n",
    "        '''3. 教師信号となるQ(s_t, a_t)値を求める'''\n",
    "\n",
    "        # 3.1 ネットワークを推論モードに切り替える\n",
    "        self.main_q_network.eval()\n",
    "        self.target_q_network.eval()\n",
    "\n",
    "        # 3.2 ネットワークが出力したQ(s_t, a_t)を求める\n",
    "        # self.model(state_batch)は、右左の両方のQ値を出力しており\n",
    "        # [torch.FloatTensor of size BATCH_SIZEx2]になっている。\n",
    "        # ここから実行したアクションa_tに対応するQ値を求めるため、action_batchで行った行動a_tが右か左かのindexを求め\n",
    "        # それに対応するQ値をgatherでひっぱり出す。\n",
    "        self.state_action_values = self.main_q_network(\n",
    "            self.state_batch).gather(1, self.action_batch)\n",
    "\n",
    "        # 3.3 max{Q(s_t+1, a)}値を求める。ただし次の状態があるかに注意。\n",
    "\n",
    "        # cartpoleがdoneになっておらず、next_stateがあるかをチェックするインデックスマスクを作成\n",
    "        non_final_mask = torch.ByteTensor(tuple(map(lambda s: s is not None,\n",
    "                                                    self.batch.next_state)))\n",
    "        # まずは全部0にしておく\n",
    "        next_state_values = torch.zeros(BATCH_SIZE)\n",
    "        a_m = torch.zeros(BATCH_SIZE).type(torch.LongTensor)\n",
    "\n",
    "        # 次の状態での最大Q値の行動a_mをMain Q-Networkから求める\n",
    "        # 最後の[1]で行動に対応したindexが返る\n",
    "        a_m[non_final_mask] = self.main_q_network(\n",
    "            self.non_final_next_states).detach().max(1)[1]\n",
    "\n",
    "        # 次の状態があるものだけにフィルターし、size 32を32×1へ\n",
    "        a_m_non_final_next_states = a_m[non_final_mask].view(-1, 1)\n",
    "\n",
    "        # 次の状態があるindexの、行動a_mのQ値をtarget Q-Networkから求める\n",
    "        # detach()で取り出す\n",
    "        # squeeze()でsize[minibatch×1]を[minibatch]に。\n",
    "        next_state_values[non_final_mask] = self.target_q_network(\n",
    "            self.non_final_next_states).gather(1, a_m_non_final_next_states).detach().squeeze()\n",
    "\n",
    "        # 3.4 教師となるQ(s_t, a_t)値を、Q学習の式から求める\n",
    "        expected_state_action_values = self.reward_batch + GAMMA * next_state_values\n",
    "\n",
    "        return expected_state_action_values\n",
    "\n",
    "    def update_main_q_network(self):\n",
    "        '''4. 結合パラメータの更新'''\n",
    "\n",
    "        # 4.1 ネットワークを訓練モードに切り替える\n",
    "        self.main_q_network.train()\n",
    "\n",
    "        # 4.2 損失関数を計算する（smooth_l1_lossはHuberloss）\n",
    "        # expected_state_action_valuesは\n",
    "        # sizeが[minbatch]になっているので、unsqueezeで[minibatch x 1]へ\n",
    "        loss = F.smooth_l1_loss(self.state_action_values,\n",
    "                                self.expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "        # 4.3 結合パラメータを更新する\n",
    "        self.optimizer.zero_grad()  # 勾配をリセット\n",
    "        loss.backward()  # バックプロパゲーションを計算\n",
    "        self.optimizer.step()  # 結合パラメータを更新\n",
    "\n",
    "    def update_target_q_network(self):  # DDQNで追加\n",
    "        '''Target Q-NetworkをMainと同じにする'''\n",
    "        self.target_q_network.load_state_dict(self.main_q_network.state_dict())\n",
    "\n",
    "    def update_td_error_memory(self):  # PrioritizedExperienceReplayで追加\n",
    "        '''TD誤差メモリに格納されているTD誤差を更新する'''\n",
    "\n",
    "        # ネットワークを推論モードに切り替える\n",
    "        self.main_q_network.eval()\n",
    "        self.target_q_network.eval()\n",
    "\n",
    "        # 全メモリでミニバッチを作成\n",
    "        transitions = self.memory.memory\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                           if s is not None])\n",
    "\n",
    "        # ネットワークが出力したQ(s_t, a_t)を求める\n",
    "        state_action_values = self.main_q_network(\n",
    "            state_batch).gather(1, action_batch)\n",
    "\n",
    "        # cartpoleがdoneになっておらず、next_stateがあるかをチェックするインデックスマスクを作成\n",
    "        non_final_mask = torch.ByteTensor(\n",
    "            tuple(map(lambda s: s is not None, batch.next_state)))\n",
    "\n",
    "        # まずは全部0にしておく、サイズはメモリの長さである\n",
    "        next_state_values = torch.zeros(len(self.memory))\n",
    "        a_m = torch.zeros(len(self.memory)).type(torch.LongTensor)\n",
    "\n",
    "        # 次の状態での最大Q値の行動a_mをMain Q-Networkから求める\n",
    "        # 最後の[1]で行動に対応したindexが返る\n",
    "        a_m[non_final_mask] = self.main_q_network(\n",
    "            non_final_next_states).detach().max(1)[1]\n",
    "\n",
    "        # 次の状態があるものだけにフィルターし、size 32を32×1へ\n",
    "        a_m_non_final_next_states = a_m[non_final_mask].view(-1, 1)\n",
    "\n",
    "        # 次の状態があるindexの、行動a_mのQ値をtarget Q-Networkから求める\n",
    "        # detach()で取り出す\n",
    "        # squeeze()でsize[minibatch×1]を[minibatch]に。\n",
    "        next_state_values[non_final_mask] = self.target_q_network(\n",
    "            non_final_next_states).gather(1, a_m_non_final_next_states).detach().squeeze()\n",
    "\n",
    "        # TD誤差を求める\n",
    "        td_errors = (reward_batch + GAMMA * next_state_values) - \\\n",
    "            state_action_values.squeeze()\n",
    "        # state_action_valuesはsize[minibatch×1]なので、squeezeしてsize[minibatch]へ\n",
    "\n",
    "        # TD誤差メモリを更新、Tensorをdetach()で取り出し、NumPyにしてから、Pythonのリストまで変換\n",
    "        self.td_error_memory.memory = td_errors.detach().numpy().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CartPoleで動くエージェントクラスです、棒付き台車そのものになります\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, num_states, num_actions):\n",
    "        '''課題の状態と行動の数を設定する'''\n",
    "        self.brain = Brain(num_states, num_actions)  # エージェントが行動を決定するための頭脳を生成\n",
    "\n",
    "    def update_q_function(self, episode):\n",
    "        '''Q関数を更新する'''\n",
    "        self.brain.replay(episode)\n",
    "\n",
    "    def get_action(self, state, episode):\n",
    "        '''行動を決定する'''\n",
    "        action = self.brain.decide_action(state, episode)\n",
    "        return action\n",
    "\n",
    "    def memorize(self, state, action, state_next, reward):\n",
    "        '''memoryオブジェクトに、state, action, state_next, rewardの内容を保存する'''\n",
    "        self.brain.memory.push(state, action, state_next, reward)\n",
    "\n",
    "    def update_target_q_function(self):\n",
    "        '''Target Q-NetworkをMain Q-Networkと同じに更新'''\n",
    "        self.brain.update_target_q_network()\n",
    "        \n",
    "    def memorize_td_error(self, td_error):  # PrioritizedExperienceReplayで追加\n",
    "        '''TD誤差メモリにTD誤差を格納'''\n",
    "        self.brain.td_error_memory.push(td_error)\n",
    "        \n",
    "    def update_td_error_memory(self):  # PrioritizedExperienceReplayで追加\n",
    "        '''TD誤差メモリに格納されているTD誤差を更新する'''\n",
    "        self.brain.update_td_error_memory()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set action variable\n",
    "# set 9 parameter\n",
    "\n",
    "# speed is 0.1 0.3 0.5\n",
    "# direction is -/8pi, -2/8pi, -3/8pi\n",
    "# temperture is 18, 22, 26\n",
    "SPEED = np.array([0.1,0.3,0.5])\n",
    "DIRECTION = np.array([-1*np.pi/8, -2*np.pi/8,-3*np.pi/8])\n",
    "T = np.array([18,22,26])\n",
    "Ux = SPEED*np.cos(DIRECTION)\n",
    "Uy = SPEED*np.sin(DIRECTION)\n",
    "#Ux**2+Uy**2  # check calculation\n",
    "U = np.array([Ux,Uy,np.zeros(3)])\n",
    "U = U.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.09238795, -0.03826834,  0.        ],\n",
       "       [ 0.21213203, -0.21213203,  0.        ],\n",
       "       [ 0.19134172, -0.46193977,  0.        ]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ux,Uy = [],[]\n",
    "for i in range(len(SPEED)):\n",
    "    for j in range(len(DIRECTION)):\n",
    "        for k in range(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-13-6221b2f8c49a>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-6221b2f8c49a>\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    self.observation_space =\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# この環境の部分を変える\n",
    "import copy\n",
    "from box import Box\n",
    "\n",
    "class Aircond():\n",
    "    '''Aircondのクラス'''\n",
    "    def __init__(self):\n",
    "        self.observation_space = \n",
    "        self.action_space = \n",
    "        \n",
    "    def reset(self):\n",
    "        '''環境のリセット'''\n",
    "        os.system('./Allclean')\n",
    "        os.system('./Makemesh')\n",
    "        \n",
    "        return observation\n",
    "    def step(self, action):\n",
    "        '''ステップを進める'''\n",
    "        \n",
    "        \n",
    "        return list(,,,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(4,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gym analysis\n",
    "env = gym.make(ENV)\n",
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00977381, -0.04072757, -0.01555263, -0.0265637 ])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation = env.reset()\n",
    "observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "action = agent.get_action(observation, episode)\n",
    "observation_next,reward,done,info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CartPoleを実行する環境のクラスです\n",
    "\n",
    "\n",
    "class Environment:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(ENV)  # 実行する課題を設定\n",
    "        num_states = self.env.observation_space.shape[0]  # 課題の状態と行動の数を設定\n",
    "        num_actions = self.env.action_space.n  # CartPoleの行動（右に左に押す）の2を取得\n",
    "        # 環境内で行動するAgentを生成\n",
    "        self.agent = Agent(num_states, num_actions)\n",
    "\n",
    "    def run(self):\n",
    "        '''実行'''\n",
    "        episode_10_list = np.zeros(10)  # 10試行分の立ち続けたstep数を格納し、平均ステップ数を出力に利用\n",
    "        complete_episodes = 0  # 195step以上連続で立ち続けた試行数\n",
    "        episode_final = False  # 最後の試行フラグ\n",
    "        frames = []  # 最後の試行を動画にするために画像を格納する変数\n",
    "\n",
    "        for episode in range(NUM_EPISODES):  # 試行数分繰り返す\n",
    "            observation = self.env.reset()  # 環境の初期化\n",
    "\n",
    "            state = observation  # 観測をそのまま状態sとして使用\n",
    "            state = torch.from_numpy(state).type(\n",
    "                torch.FloatTensor)  # numpy変数をPyTorchのテンソルに変換\n",
    "            state = torch.unsqueeze(state, 0)  # size 4をsize 1x4に変換\n",
    "\n",
    "            for step in range(MAX_STEPS):  # 1エピソードのループ\n",
    "\n",
    "                # 動画描画をコメントアウトしています\n",
    "                # if episode_final is True:  # 最終試行ではframesに各時刻の画像を追加していく\n",
    "                    # frames.append(self.env.render(mode='rgb_array'))\n",
    "\n",
    "                action = self.agent.get_action(state, episode)  # 行動を求める\n",
    "\n",
    "                # 行動a_tの実行により、s_{t+1}とdoneフラグを求める\n",
    "                # actionから.item()を指定して、中身を取り出す\n",
    "                observation_next, _, done, _ = self.env.step(\n",
    "                    action.item())  # rewardとinfoは使わないので_にする\n",
    "\n",
    "                # 報酬を与える。さらにepisodeの終了評価と、state_nextを設定する\n",
    "                if done:  # ステップ数が200経過するか、一定角度以上傾くとdoneはtrueになる\n",
    "                    state_next = None  # 次の状態はないので、Noneを格納\n",
    "\n",
    "                    # 直近10episodeの立てたstep数リストに追加\n",
    "                    episode_10_list = np.hstack(\n",
    "                        (episode_10_list[1:], step + 1))\n",
    "\n",
    "                    if step < 195:\n",
    "                        reward = torch.FloatTensor(\n",
    "                            [-1.0])  # 途中でこけたら罰則として報酬-1を与える\n",
    "                        complete_episodes = 0  # 連続成功記録をリセット\n",
    "                    else:\n",
    "                        reward = torch.FloatTensor([1.0])  # 立ったまま終了時は報酬1を与える\n",
    "                        complete_episodes = complete_episodes + 1  # 連続記録を更新\n",
    "                else:\n",
    "                    reward = torch.FloatTensor([0.0])  # 普段は報酬0\n",
    "                    state_next = observation_next  # 観測をそのまま状態とする\n",
    "                    state_next = torch.from_numpy(state_next).type(\n",
    "                        torch.FloatTensor)  # numpy変数をPyTorchのテンソルに変換\n",
    "                    state_next = torch.unsqueeze(\n",
    "                        state_next, 0)  # size 4をsize 1x4に変換\n",
    "\n",
    "                # メモリに経験を追加\n",
    "                self.agent.memorize(state, action, state_next, reward)\n",
    "\n",
    "                # TD誤差メモリにTD誤差を追加\n",
    "                self.agent.memorize_td_error(0)  # 本当はTD誤差を格納するが、0をいれておく\n",
    "\n",
    "                # PrioritizedExperienceReplayでQ関数を更新する\n",
    "                self.agent.update_q_function(episode)\n",
    "\n",
    "                # 観測の更新\n",
    "                state = state_next\n",
    "\n",
    "                # 終了時の処理\n",
    "                if done:\n",
    "                    print('%d Episode: Finished after %d steps：10試行の平均step数 = %.1lf' % (\n",
    "                        episode, step + 1, episode_10_list.mean()))\n",
    "\n",
    "                    # TD誤差メモリの中身を更新する\n",
    "                    self.agent.update_td_error_memory()\n",
    "\n",
    "                    # DDQNで追加、2試行に1度、Target Q-NetworkをMainと同じにコピーする\n",
    "                    if(episode % 2 == 0):\n",
    "                        self.agent.update_target_q_function()\n",
    "                    break\n",
    "\n",
    "            if episode_final is True:\n",
    "                # 動画描画をコメントアウトしています\n",
    "                # 動画を保存と描画\n",
    "                # display_frames_as_gif(frames)\n",
    "                break\n",
    "\n",
    "            # 10連続で200step経ち続けたら成功\n",
    "            if complete_episodes >= 10:\n",
    "                print('10回連続成功')\n",
    "                episode_final = True  # 次の試行を描画を行う最終試行とする\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=4, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=2, bias=True)\n",
      ")\n",
      "0 Episode: Finished after 20 steps：10試行の平均step数 = 2.0\n",
      "1 Episode: Finished after 12 steps：10試行の平均step数 = 3.2\n",
      "2 Episode: Finished after 10 steps：10試行の平均step数 = 4.2\n",
      "3 Episode: Finished after 13 steps：10試行の平均step数 = 5.5\n",
      "4 Episode: Finished after 9 steps：10試行の平均step数 = 6.4\n",
      "5 Episode: Finished after 12 steps：10試行の平均step数 = 7.6\n",
      "6 Episode: Finished after 10 steps：10試行の平均step数 = 8.6\n",
      "7 Episode: Finished after 10 steps：10試行の平均step数 = 9.6\n",
      "8 Episode: Finished after 12 steps：10試行の平均step数 = 10.8\n",
      "9 Episode: Finished after 14 steps：10試行の平均step数 = 12.2\n",
      "10 Episode: Finished after 23 steps：10試行の平均step数 = 12.5\n",
      "11 Episode: Finished after 10 steps：10試行の平均step数 = 12.3\n",
      "12 Episode: Finished after 10 steps：10試行の平均step数 = 12.3\n",
      "13 Episode: Finished after 11 steps：10試行の平均step数 = 12.1\n",
      "14 Episode: Finished after 10 steps：10試行の平均step数 = 12.2\n",
      "15 Episode: Finished after 9 steps：10試行の平均step数 = 11.9\n",
      "16 Episode: Finished after 10 steps：10試行の平均step数 = 11.9\n",
      "17 Episode: Finished after 9 steps：10試行の平均step数 = 11.8\n",
      "18 Episode: Finished after 11 steps：10試行の平均step数 = 11.7\n",
      "19 Episode: Finished after 9 steps：10試行の平均step数 = 11.2\n",
      "20 Episode: Finished after 11 steps：10試行の平均step数 = 10.0\n",
      "21 Episode: Finished after 9 steps：10試行の平均step数 = 9.9\n",
      "22 Episode: Finished after 10 steps：10試行の平均step数 = 9.9\n",
      "23 Episode: Finished after 10 steps：10試行の平均step数 = 9.8\n",
      "24 Episode: Finished after 10 steps：10試行の平均step数 = 9.8\n",
      "25 Episode: Finished after 8 steps：10試行の平均step数 = 9.7\n",
      "26 Episode: Finished after 8 steps：10試行の平均step数 = 9.5\n",
      "27 Episode: Finished after 8 steps：10試行の平均step数 = 9.4\n",
      "28 Episode: Finished after 10 steps：10試行の平均step数 = 9.3\n",
      "29 Episode: Finished after 10 steps：10試行の平均step数 = 9.4\n",
      "30 Episode: Finished after 10 steps：10試行の平均step数 = 9.3\n",
      "31 Episode: Finished after 10 steps：10試行の平均step数 = 9.4\n",
      "32 Episode: Finished after 10 steps：10試行の平均step数 = 9.4\n",
      "33 Episode: Finished after 9 steps：10試行の平均step数 = 9.3\n",
      "34 Episode: Finished after 10 steps：10試行の平均step数 = 9.3\n",
      "35 Episode: Finished after 11 steps：10試行の平均step数 = 9.6\n",
      "36 Episode: Finished after 11 steps：10試行の平均step数 = 9.9\n",
      "37 Episode: Finished after 11 steps：10試行の平均step数 = 10.2\n",
      "38 Episode: Finished after 9 steps：10試行の平均step数 = 10.1\n",
      "39 Episode: Finished after 9 steps：10試行の平均step数 = 10.0\n",
      "40 Episode: Finished after 9 steps：10試行の平均step数 = 9.9\n",
      "41 Episode: Finished after 9 steps：10試行の平均step数 = 9.8\n",
      "42 Episode: Finished after 10 steps：10試行の平均step数 = 9.8\n",
      "43 Episode: Finished after 11 steps：10試行の平均step数 = 10.0\n",
      "44 Episode: Finished after 9 steps：10試行の平均step数 = 9.9\n",
      "45 Episode: Finished after 9 steps：10試行の平均step数 = 9.7\n",
      "46 Episode: Finished after 9 steps：10試行の平均step数 = 9.5\n",
      "47 Episode: Finished after 12 steps：10試行の平均step数 = 9.6\n",
      "48 Episode: Finished after 10 steps：10試行の平均step数 = 9.7\n",
      "49 Episode: Finished after 11 steps：10試行の平均step数 = 9.9\n",
      "50 Episode: Finished after 10 steps：10試行の平均step数 = 10.0\n",
      "51 Episode: Finished after 11 steps：10試行の平均step数 = 10.2\n",
      "52 Episode: Finished after 15 steps：10試行の平均step数 = 10.7\n",
      "53 Episode: Finished after 12 steps：10試行の平均step数 = 10.8\n",
      "54 Episode: Finished after 11 steps：10試行の平均step数 = 11.0\n",
      "55 Episode: Finished after 11 steps：10試行の平均step数 = 11.2\n",
      "56 Episode: Finished after 17 steps：10試行の平均step数 = 12.0\n",
      "57 Episode: Finished after 12 steps：10試行の平均step数 = 12.0\n",
      "58 Episode: Finished after 11 steps：10試行の平均step数 = 12.1\n",
      "59 Episode: Finished after 33 steps：10試行の平均step数 = 14.3\n",
      "60 Episode: Finished after 25 steps：10試行の平均step数 = 15.8\n",
      "61 Episode: Finished after 34 steps：10試行の平均step数 = 18.1\n",
      "62 Episode: Finished after 26 steps：10試行の平均step数 = 19.2\n",
      "63 Episode: Finished after 41 steps：10試行の平均step数 = 22.1\n",
      "64 Episode: Finished after 59 steps：10試行の平均step数 = 26.9\n",
      "65 Episode: Finished after 34 steps：10試行の平均step数 = 29.2\n",
      "66 Episode: Finished after 51 steps：10試行の平均step数 = 32.6\n",
      "67 Episode: Finished after 10 steps：10試行の平均step数 = 32.4\n",
      "68 Episode: Finished after 34 steps：10試行の平均step数 = 34.7\n",
      "69 Episode: Finished after 31 steps：10試行の平均step数 = 34.5\n",
      "70 Episode: Finished after 42 steps：10試行の平均step数 = 36.2\n",
      "71 Episode: Finished after 39 steps：10試行の平均step数 = 36.7\n",
      "72 Episode: Finished after 51 steps：10試行の平均step数 = 39.2\n",
      "73 Episode: Finished after 44 steps：10試行の平均step数 = 39.5\n",
      "74 Episode: Finished after 55 steps：10試行の平均step数 = 39.1\n",
      "75 Episode: Finished after 23 steps：10試行の平均step数 = 38.0\n",
      "76 Episode: Finished after 64 steps：10試行の平均step数 = 39.3\n",
      "77 Episode: Finished after 26 steps：10試行の平均step数 = 40.9\n",
      "78 Episode: Finished after 31 steps：10試行の平均step数 = 40.6\n",
      "79 Episode: Finished after 41 steps：10試行の平均step数 = 41.6\n",
      "80 Episode: Finished after 44 steps：10試行の平均step数 = 41.8\n",
      "81 Episode: Finished after 23 steps：10試行の平均step数 = 40.2\n",
      "82 Episode: Finished after 25 steps：10試行の平均step数 = 37.6\n",
      "83 Episode: Finished after 27 steps：10試行の平均step数 = 35.9\n",
      "84 Episode: Finished after 44 steps：10試行の平均step数 = 34.8\n",
      "85 Episode: Finished after 36 steps：10試行の平均step数 = 36.1\n",
      "86 Episode: Finished after 33 steps：10試行の平均step数 = 33.0\n",
      "87 Episode: Finished after 25 steps：10試行の平均step数 = 32.9\n",
      "88 Episode: Finished after 29 steps：10試行の平均step数 = 32.7\n",
      "89 Episode: Finished after 27 steps：10試行の平均step数 = 31.3\n",
      "90 Episode: Finished after 36 steps：10試行の平均step数 = 30.5\n",
      "91 Episode: Finished after 23 steps：10試行の平均step数 = 30.5\n",
      "92 Episode: Finished after 36 steps：10試行の平均step数 = 31.6\n",
      "93 Episode: Finished after 26 steps：10試行の平均step数 = 31.5\n",
      "94 Episode: Finished after 31 steps：10試行の平均step数 = 30.2\n",
      "95 Episode: Finished after 24 steps：10試行の平均step数 = 29.0\n",
      "96 Episode: Finished after 33 steps：10試行の平均step数 = 29.0\n",
      "97 Episode: Finished after 32 steps：10試行の平均step数 = 29.7\n",
      "98 Episode: Finished after 32 steps：10試行の平均step数 = 30.0\n",
      "99 Episode: Finished after 27 steps：10試行の平均step数 = 30.0\n",
      "100 Episode: Finished after 25 steps：10試行の平均step数 = 28.9\n",
      "101 Episode: Finished after 32 steps：10試行の平均step数 = 29.8\n",
      "102 Episode: Finished after 34 steps：10試行の平均step数 = 29.6\n",
      "103 Episode: Finished after 52 steps：10試行の平均step数 = 32.2\n",
      "104 Episode: Finished after 42 steps：10試行の平均step数 = 33.3\n",
      "105 Episode: Finished after 34 steps：10試行の平均step数 = 34.3\n",
      "106 Episode: Finished after 27 steps：10試行の平均step数 = 33.7\n",
      "107 Episode: Finished after 60 steps：10試行の平均step数 = 36.5\n",
      "108 Episode: Finished after 35 steps：10試行の平均step数 = 36.8\n",
      "109 Episode: Finished after 26 steps：10試行の平均step数 = 36.7\n",
      "110 Episode: Finished after 38 steps：10試行の平均step数 = 38.0\n",
      "111 Episode: Finished after 36 steps：10試行の平均step数 = 38.4\n",
      "112 Episode: Finished after 29 steps：10試行の平均step数 = 37.9\n",
      "113 Episode: Finished after 35 steps：10試行の平均step数 = 36.2\n",
      "114 Episode: Finished after 34 steps：10試行の平均step数 = 35.4\n",
      "115 Episode: Finished after 36 steps：10試行の平均step数 = 35.6\n",
      "116 Episode: Finished after 74 steps：10試行の平均step数 = 40.3\n",
      "117 Episode: Finished after 28 steps：10試行の平均step数 = 37.1\n",
      "118 Episode: Finished after 32 steps：10試行の平均step数 = 36.8\n",
      "119 Episode: Finished after 34 steps：10試行の平均step数 = 37.6\n",
      "120 Episode: Finished after 47 steps：10試行の平均step数 = 38.5\n",
      "121 Episode: Finished after 36 steps：10試行の平均step数 = 38.5\n",
      "122 Episode: Finished after 24 steps：10試行の平均step数 = 38.0\n",
      "123 Episode: Finished after 42 steps：10試行の平均step数 = 38.7\n",
      "124 Episode: Finished after 50 steps：10試行の平均step数 = 40.3\n",
      "125 Episode: Finished after 28 steps：10試行の平均step数 = 39.5\n",
      "126 Episode: Finished after 33 steps：10試行の平均step数 = 35.4\n",
      "127 Episode: Finished after 35 steps：10試行の平均step数 = 36.1\n",
      "128 Episode: Finished after 38 steps：10試行の平均step数 = 36.7\n",
      "129 Episode: Finished after 56 steps：10試行の平均step数 = 38.9\n",
      "130 Episode: Finished after 74 steps：10試行の平均step数 = 41.6\n",
      "131 Episode: Finished after 38 steps：10試行の平均step数 = 41.8\n",
      "132 Episode: Finished after 121 steps：10試行の平均step数 = 51.5\n",
      "133 Episode: Finished after 200 steps：10試行の平均step数 = 67.3\n",
      "134 Episode: Finished after 62 steps：10試行の平均step数 = 68.5\n",
      "135 Episode: Finished after 55 steps：10試行の平均step数 = 71.2\n",
      "136 Episode: Finished after 55 steps：10試行の平均step数 = 73.4\n",
      "137 Episode: Finished after 48 steps：10試行の平均step数 = 74.7\n",
      "138 Episode: Finished after 73 steps：10試行の平均step数 = 78.2\n",
      "139 Episode: Finished after 51 steps：10試行の平均step数 = 77.7\n",
      "140 Episode: Finished after 45 steps：10試行の平均step数 = 74.8\n",
      "141 Episode: Finished after 118 steps：10試行の平均step数 = 82.8\n",
      "142 Episode: Finished after 59 steps：10試行の平均step数 = 76.6\n",
      "143 Episode: Finished after 58 steps：10試行の平均step数 = 62.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144 Episode: Finished after 88 steps：10試行の平均step数 = 65.0\n",
      "145 Episode: Finished after 144 steps：10試行の平均step数 = 73.9\n",
      "146 Episode: Finished after 120 steps：10試行の平均step数 = 80.4\n",
      "147 Episode: Finished after 177 steps：10試行の平均step数 = 93.3\n",
      "148 Episode: Finished after 106 steps：10試行の平均step数 = 96.6\n",
      "149 Episode: Finished after 200 steps：10試行の平均step数 = 111.5\n",
      "150 Episode: Finished after 63 steps：10試行の平均step数 = 113.3\n",
      "151 Episode: Finished after 99 steps：10試行の平均step数 = 111.4\n",
      "152 Episode: Finished after 200 steps：10試行の平均step数 = 125.5\n",
      "153 Episode: Finished after 200 steps：10試行の平均step数 = 139.7\n",
      "154 Episode: Finished after 60 steps：10試行の平均step数 = 136.9\n",
      "155 Episode: Finished after 54 steps：10試行の平均step数 = 127.9\n",
      "156 Episode: Finished after 200 steps：10試行の平均step数 = 135.9\n",
      "157 Episode: Finished after 200 steps：10試行の平均step数 = 138.2\n",
      "158 Episode: Finished after 200 steps：10試行の平均step数 = 147.6\n",
      "159 Episode: Finished after 166 steps：10試行の平均step数 = 144.2\n",
      "160 Episode: Finished after 200 steps：10試行の平均step数 = 157.9\n",
      "161 Episode: Finished after 174 steps：10試行の平均step数 = 165.4\n",
      "162 Episode: Finished after 200 steps：10試行の平均step数 = 165.4\n",
      "163 Episode: Finished after 200 steps：10試行の平均step数 = 165.4\n",
      "164 Episode: Finished after 200 steps：10試行の平均step数 = 179.4\n",
      "165 Episode: Finished after 180 steps：10試行の平均step数 = 192.0\n",
      "166 Episode: Finished after 200 steps：10試行の平均step数 = 192.0\n",
      "167 Episode: Finished after 200 steps：10試行の平均step数 = 192.0\n",
      "168 Episode: Finished after 200 steps：10試行の平均step数 = 192.0\n",
      "169 Episode: Finished after 200 steps：10試行の平均step数 = 195.4\n",
      "170 Episode: Finished after 200 steps：10試行の平均step数 = 195.4\n",
      "171 Episode: Finished after 200 steps：10試行の平均step数 = 198.0\n",
      "172 Episode: Finished after 200 steps：10試行の平均step数 = 198.0\n",
      "173 Episode: Finished after 200 steps：10試行の平均step数 = 198.0\n",
      "174 Episode: Finished after 200 steps：10試行の平均step数 = 198.0\n",
      "175 Episode: Finished after 200 steps：10試行の平均step数 = 200.0\n",
      "10回連続成功\n",
      "176 Episode: Finished after 200 steps：10試行の平均step数 = 200.0\n"
     ]
    }
   ],
   "source": [
    "# main クラス\n",
    "cartpole_env = Environment()\n",
    "cartpole_env.run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
